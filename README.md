# XAI-Visual-Guts
 
This repository archives the artistic research results from the production grants Leonardo Rebooted: [https://quoartis.org/project/leonardo-rebooted/](https://quoartis.org/project/leonardo-rebooted/); within the category of Artificial Intelligence and Quantum Computing.

For details:  
- Setting up for machine learning and training - [ML_Training](./ML_Training.md)
- Dataset gathering and processing - [ML_Datasets](./ML_Datasets.md)
- Setting up for real-time machine learning  - [ML_Realtime_Setup](./ML_Realtime_Setup.md)
- How to use real-time machine learning  - [ML_Realtime](./ML_Realtime.md)
- Final thoughts - [ML_PostMortem](./ML_PostMortem)

# Pitch

## Abstract
Artificial intelligence and machine learning (AIML) are rapidly changing nearly every aspect of post-modern life; but, do we really understand it? Quantum Computing will only accelerate the adoption of AIML as the new algorithm(s) for universal computability. Explainable AI (XAI) is a burgeoning field of research with the goal of opening the black-box of AIML and explaining how it works. This is typically from the technical standpoint of calculating and demonstrating input to output connections with the hope of generating more trust in AIML. The intent of this proposal is to create and reveal the inner workings of an AIML from a visual arts standpoint - visual arts XAI.

## Concept
Create a short audiovisual video/animation from AIML trained by the artist that self- exposes latent space navigation and neural net layer interaction. Ideally, the final AIML agent(s) will be fully autonomous - meaning that audiovisuals could be generated by the AIML agent infinitely. Additionally, the artist will publish/open source any code, discoveries, and how-to instructions on github. Philosophical musings and written products may occur and primarily framed within Flusser's concept of the Technical Image.



# Updates

## First update

This was a proof of concept with some data capture, NN training, and loading the NN into Touch Designer. Then, it shows how to select and use synthesis layers in real-time.

[![Demo of real-time Stylegan 3](./Images/XAI_First_Update.png)](https://vimeo.com/728939660 "Demo of real-time Stylegan 3")


## Second update

This demonstes how to load up multiple NNs and linearly interpolate (LERP) or blend specific tensor layer(s) between two different NNs. This lead to entirely swapping out the layer of one NN with the equivalent layer of second NN. This video starts with a couple seconds of one NN, then a couple seconds of the second NN. The remainder of the video is showing various treatments when LERPing tensor layers.

[![Swapping tensor layers in real-time between two NNs](./Images/ML_LayerSwap.png)](https://vimeo.com/728940929 "Swapping tensor layers in real-time between two NNs")




## Third update

This video shows five different people, vertically, at various degrees of completed NN training, horizonally, with final image, input, L14, L10, L6, and L0 synthesis layers all with the exact same NN input data.

[![Layer states of machine learning in training](./Images/all_five.PNG)](https://vimeo.com/727455868 "Layer states of machine learning in training")




## Final update

Animations were created and sent to the musicians. They made several recordings in a live cinema method - improvisational performance with projected video. The recordings were then used and analyzed and used to manipulation parameters of the NN input generators. Several visual recordings were made and edited together with titles and audio.




# Credits

Vilolist - [Chris Fisher-Lochhead](http://cflmusic.com)
Electronics (EIS) - [Michael Century](http://nextcentury.ca)
Audio Engineer - [Ross Rice](https://feistyfishrr.wixsite.com/rossarice)
EIS - Expanded Instrument System, with the Permission of The Pauline Oliveros Trust and The Ministry of Maat. [https://www.ministryofmaat.org](https://www.ministryofmaat.org)

Video Capture - [Jeremy Stewart](https://blindelephants.co)
Storytellers  -  , , ,

Machine Learning Hardware Support - [ASU Research Computing](https://cores.research.asu.edu/research-computing/about)

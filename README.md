# XAI-Visual-Guts
 
This repository archives the artistic research results from the production grants Leonardo Rebooted: [https://quoartis.org/project/leonardo-rebooted/](https://quoartis.org/project/leonardo-rebooted/); within the category of Artificial Intelligence and Quantum Computing.

For details:  
- Setting up for machine learning and training - [ML_Training](./ML_Training.md)
- Dataset gathering and processing - [ML_Datasets](./ML_Datasets.md)
- Setting up for real-time machine learning  - [ML_Realtime_Setup](./ML_Realtime_Setup.md)
- How to use real-time machine learning  - [ML_Realtime](./ML_Realtime.md)


# Pitch

## Abstract
Artificial intelligence and machine learning (AIML) are rapidly changing nearly every aspect of post-modern life; but, do we really understand it? Quantum Computing will only accelerate the adoption of AIML as the new algorithm(s) for universal computability. Explainable AI (XAI) is a burgeoning field of research with the goal of opening the black-box of AIML and explaining how it works. This is typically from the technical standpoint of calculating and demonstrating input to output connections with the hope of generating more trust in AIML. The intent of this proposal is to create and reveal the inner workings of an AIML from a visual arts standpoint - visual arts XAI.

## Concept
Create a short audiovisual video/animation from AIML trained by the artist that self- exposes latent space navigation and neural net layer interaction. Ideally, the final AIML agent(s) will be fully autonomous - meaning that audiovisuals could be generated by the AIML agent infinitely. Additionally, the artist will publish/open source any code, discoveries, and how-to instructions on github. Philosophical musings and written products may occur and primarily framed within Flusser's concept of the Technical Image.



# Updates

## First update

This was a proof of concept of data capture, training, and loading the NN into Touch Designer.

[![Demo of real-time Stylegan 3](./XAI_First_Update.png)](https://vimeo.com/728939660 "Demo of real-time Stylegan 3")


## Second update

This was demonstrating how to load up multiple NNs and linearly interpolate (LERP) or blend specific layers between two different NNs. Leading to entirely swapping out the layer of one NN with the equivalent layer of second NN. 

[![Swapping tensor layers in real-time between two NNs](./ML_LayerSwap.png)](https://vimeo.com/728940929 "Swapping tensor layers in real-time between two NNs")




## Third update

Grid video showing three different people with final image, input, L14, L10, L6, and L0 synthesis layers all with the exact same NN input data. 

[![Layer states of machine learning in training](./all_five.PNG)](https://vimeo.com/727455868 "Layer states of machine learning in training")




## Final update

Animations were created and sent to the musicians. They made several recordings in a live cinema




# Credits

Composer - [Michael Century](http://nextcentury.ca)
Vilolist - [Chris Fisher-Lochhead](http://cflmusic.com)
Audio Engineer - [Ross Rice](https://feistyfishrr.wixsite.com/rossarice)

Video Capture - [Jeremy Stewart](https://blindelephants.co)
Storytellers  -  , , ,

Machine Learning - [ASU Research Computing](https://cores.research.asu.edu/research-computing/about)
